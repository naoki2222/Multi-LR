{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import makeindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pca_vector(all_vector, n_components):\n",
    "    pca = PCA(n_components = n_components)\n",
    "    all_vector_pca = pca.transform(all_vector)\n",
    "    print('寄与率 : ' + str(sum(pca.explained_variance_ratio_)))\n",
    "    return all_vector_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, val):\n",
    "    return [k for k, v in d.items() if v == val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存するファイルの名前\n",
    "model_name = 'model_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み \n",
    "with open('data/commit_files.bin', 'rb') as f:\n",
    "    commit_files = pickle.load(f) # load commit_set\n",
    "\n",
    "#データの読み込み (文章ベクトル)\n",
    "with open('data/all_vector.bin', 'rb') as f:\n",
    "    all_vector = pickle.load(f) # load commit_set\n",
    "\n",
    "with open('data/vector_set.bin', 'rb') as f:\n",
    "    vector_set = pickle.load(f) # load commit_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcaの実行\n",
    "all_vector = make_pca_vector(all_vector, n_components=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトルをnumpy, tensor型へ\n",
    "sentence_vector = np.array(all_vector)\n",
    "sentence_tensor = tf.cast(sentence_vector, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイルとアイテムにインデックスを振り分ける。\n",
    "file_index, file_index_list = makeindex.make_file_index_list(commit_files)\n",
    "version_index = makeindex.make_version_index(commit_files)\n",
    "vector_index_set = makeindex.make_vector_index_set(vector_set)\n",
    "Fi_num = makeindex.make_Fi_num(commit_files)\n",
    "z_tensor = makeindex.z_init(file_index, file_index_list, all_vector, vector_index_set, Fi_num)\n",
    "z_index = [(z1, z2)\n",
    "                for z1 in range(z_tensor.shape[0])\n",
    "                for z2 in range(z_tensor.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数\n",
    "def loss_func(x, z, model):\n",
    "    bx = tf.matmul(model.B , np.array(x).T)\n",
    "    T_bx = tf.transpose(bx)\n",
    "    log_p = tf.transpose(tf.nn.log_softmax(T_bx))\n",
    "    return -tf.math.reduce_sum(log_p*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, sentence_vector, z):\n",
    "        m = z.shape[0]\n",
    "        self.B = tf.Variable(tf.ones([m,len(sentence_vector[0])]))\n",
    "        \n",
    "    def __call__(self, sentence_vector):\n",
    "        bx_matrix = tf.matmul(self.B , np.array(sentence_vector).T)\n",
    "        T_bx_matrix = tf.transpose(bx_matrix)\n",
    "        p_matrix = tf.transpose(tf.nn.softmax(T_bx_matrix))\n",
    "        return  p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zの更新\n",
    "def update_z(x, z, model, file_index, file_index_list, vector_index_set, z_index):\n",
    "    new_z = np.ones(z.shape)\n",
    "    bx = tf.matmul(model.B , np.array(x).T)\n",
    "    T_bx = tf.transpose(bx)\n",
    "    p = np.array(tf.transpose(tf.nn.softmax(T_bx)))\n",
    "\n",
    "    for z1,z2 in z_index:\n",
    "        F_i = file_index_list[vector_index_set[z2][0]]\n",
    "        if z1 in F_i:\n",
    "            sum_p = p.T[z2][F_i].sum()\n",
    "            new_z[z1][z2] = p[z1][z2] / sum_p\n",
    "        else:\n",
    "            new_z[z1][z2] = 0\n",
    "    return new_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamで回帰係数の最適化(10000回)\n",
    "def train(model, x, z):\n",
    "    for i in range(10000):\n",
    "        optimizer = tf.optimizers.Adam(0.001)\n",
    "        with tf.GradientTape() as t:\n",
    "            current_loss = loss_func(x, z, model)\n",
    "            if torch.isnan(torch.tensor([loss_func(sentence_tensor, z_tensor, model).numpy()])):\n",
    "                print(i)\n",
    "                print('nan')\n",
    "                break\n",
    "            grads = t.gradient(current_loss, [model.B])\n",
    "            optimizer.apply_gradients(zip(grads, [model.B]))\n",
    "            \n",
    "            if abs(loss_func(x, z, model).numpy() - current_loss.numpy()) <= 1:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル生成\n",
    "model = Model(sentence_vector, z_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習の実行(100回)\n",
    "for i in tqdm(range(100)):\n",
    "    train(model, sentence_tensor, z_tensor)\n",
    "    new_z = update_z(sentence_tensor, z_tensor, model, file_index, file_index_list, vector_index_set, z_index)\n",
    "    z_tensor = tf.cast(new_z, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#確率Pの計算\n",
    "pred_p = model(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/z_'+model_name+'.bin', 'wb') as f:\n",
    "    pickle.dump(z_tensor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/'+model_name+'.bin', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
